Doc2Query--: When Less is MoreMitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2University of Glasgow12024810G@student.gla.ac.uk2{first}.{last}@glasgow.ac.ukAbstract. Doc2Query — the process of expanding the content of adocument before indexing using a sequence-to-sequence model — hasemerged as a prominent technique for improving the ﬁrst-stage retrievaleﬀectivenessofsearchengines.However,sequence-to-sequencemodelsareknown to be prone to “hallucinating” content that is not present in thesource text. We argue that Doc2Query is indeed prone to hallucination,which ultimately harms retrieval eﬀectiveness and inﬂates the index size.In this work, we explore techniques for ﬁltering out these harmful queriesprior to indexing. We ﬁnd that using a relevance model to remove poor-quality queries can improve the retrieval eﬀectiveness of Doc2Query byup to 16%, while simultaneously reducing mean query execution time by23% and cutting the index size by 33%. We release the code, data, anda live demonstration to facilitate reproduction and further exploration.11 IntroductionNeural network models, particularly those based on contextualised languagemodels, have been shown to improve search eﬀectiveness [3]. While some ap-proaches focus on re-ranking document sets from a ﬁrst-stage retrieval functionto improve precision [27], others aim to improve the ﬁrst stage itself [4]. In thiswork, we focus on one of these ﬁrst-stage approaches: Doc2Query [29]. This ap-proach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries thatmay be relevant to a particular text. Then, when indexing, this model is usedtoexpandthe document by generating a collection of queries and appendingthem to the document. Though computationally expensive at index time [34],this approach has been shown to be remarkably eﬀective even when retrievingusing simple lexical models like BM25 [28]. Numerous works have shown thatthe approach can produce a high-quality pool of results that are eﬀective forsubsequent stages in the ranking pipeline [19,20,23,40].However, sequence-to-sequence models are well-known to be prone to gener-ate content that does not reﬂect the input text – a defect known in literatureas “hallucination” [25]. We ﬁnd that existing Doc2Query models are no excep-tion. Figure 1 provides example generated queries from the state-of-the-art T5Doc2Query model [28]. In this example, we see that many of the generatedqueries cannot actually be answered by the source passage (score 1).1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 20232 Gospodinov et al.Original Passage: Barley (Hordeum vulgare L.), amember of the grass family, is a major cereal grain. Itwas one of the ﬁrst cultivated grains and is now grownwidely. Barley grain is a staple in Tibetan cuisine andwaseatenwidelybypeasantsinMedievalEurope.Bar-ley has also been used as animal fodder, as a sourceof fermentable material for beer and certain distilledbeverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originatefrom(2) what is the name of the cereal grain usedin tibetan cooking? (3) what is barley used for (1)what is barley in food (0) what is bare wheat (3)what family of organisms is barley in (1) why is bar-ley important in tibetan diet (3) what is barley (2) where is barley grown (1) where was barley ﬁrstgrown and eaten (1) where was barley ﬁrst used ...Fig. 1.Example passage from MS MARCO and generated queries using the T5Doc2Query model. The relevance of each query to the passage is scored by the au-thors on a scale of 0–3 using the TREC Deep Learning passage relevance criteria.Based on this observation, we hypothesise that retrieval performance ofDoc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,weconduct experiments where we apply a new ﬁltering phase that aims to removepoor queries prior to indexing. Given that this approach removes queries, wecall the approach Doc2Query-- (Doc2Query-minus-minus). Rather than traininga new model for this task, we identify that relevance models are already ﬁt forthis purpose: they estimate how relevant a passage is to a query. We thereforeexplore ﬁltering strategies that make use of existing neural relevance models.Through experimentation on the MS MARCO dataset, we ﬁnd that our ﬁl-tering approach can improve the retrieval eﬀectiveness of indexes built usingDoc2Query-- by up to 16%; less can indeed be more. Meanwhile, ﬁltering nat-urally reduces the index size, lowering storage and query-time computationalcosts. Finally, we conduct an exploration of the index-time overheads introducedbytheﬁlteringprocessandconcludethatthegainsfromﬁlteringmorethanmakeup for the additional time spent generating more queries. The approach also hasa positive impact on the environmental costs of applying Doc2Query; the sameretrieval eﬀectiveness can be achieved with only about a third of the compu-tational cost when indexing. To facilitate last-metre, last-mile, and completereproduction eﬀorts [36], we release the code, indices, and ﬁltering scores.1Insummary, we contribute a technique to improve the eﬀectiveness and eﬃciencyof Doc2Query by ﬁltering out queries that do not reﬂect the original passage.2 Related WorkThe classical lexical mismatch problem is a key one in information retrieval -documents that do not contain the query terms may not be retrieved. In theliterature, various approaches have addressed this: query reformulation – includ-ing stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) – anddocument expansion [9,30,35]. Classically, query expansion models have beenpopular, as they avoid the costs associated with making additional processingfor each document needed for document expansion. However, query expansionmay result in reduced performance [11], as queries are typically short and thenecessary evidence to understand the context of the user is limited.Doc2Query--: When Less is More 3The application of latent representations of queries and documents, suchas using latent semantic indexing [8] allow retrieval to not be driven directlyby lexical signals. More recently, transformer-based language models (such asBERT [6]) have resulted in representations of text where the contextualisedmeaning of words are accounted for. In particular, in dense retrieval, queriesand documents are represented in embeddings spaces [14,37], often facilitatedby Approximate Nearest Neighbour (ANN) data structures [13]. However, evenwhen using ANN, retrieval can still be ineﬃcient or insuﬃciently eﬀective [15].Others have explored approaches for augmenting lexical representations withadditional terms that may be relevant. In this work, we explore Doc2Query [29],which uses a sequence-to-sequence model that maps a document to queries thatit might be able to answer. By appending these generated queries to a docu-ment’s content before indexing, the document is more likely to be retrieved foruser queries when using a model like BM25. An alternative style of documentexpansion, proposed by MacAvaney et al. [19] and since used by several othermodels (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)mechanism. MLM expansion generates individual tokens to append to the docu-ment as a bag of words (rather than as a sequence). Although MLM expansion isalso prone to hallucination,2the bag-of-words nature of MLM expansion meansthat individual expansion tokens may not have suﬃcient context to apply ﬁl-tering eﬀectively. We therefore focus only on sequence-style expansion and leavethe exploration of MLM expansion for future work.3 Doc2Query--Doc2Query-- consists of two phases: a generation phrase and a ﬁltering phase.In the generation phase, a Doc2Query model generates a set of nqueries thateach document might be able to answer. However, as shown in Figure 1, notall of the queries are necessarily relevant to the document. To mitigate thisproblem, Doc2Query-- then proceeds to a ﬁltering phase, which is responsiblefor eliminating the generated queries that are least relevant to the source doc-ument. Because hallucinated queries contain details not present in the originaltext (by deﬁnition), we argue that hallucinated queries are less useful for re-trieval than non-hallucinated ones. Filtering is accomplished by retaining onlythe most relevant pproportion of generated queries over the entire corpus. Theretained queries are then concatenated to their corresponding documents priorto indexing, as per the existing Doc2Query approach.More formally, consider an expansion function ethat maps a document to nqueries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-natedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))jd2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsaﬁlteringmechanism that uses a relevance model that maps a query and document to areal-valued relevance score s:QD7!R(with larger values indicating higher2For instance, we ﬁnd that SPLADE [10] generates the following seemingly-unrelatedterms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.4 Gospodinov et al.relevance). The relevance scoring function is used to ﬁlter down the queries tothose that meet a certain score threshold tas follows:D0=nConcat d;qjq2e(d)^s(q; d)t	jd2Do(1)The relevance threshold tis naturally dependent upon the relevance scoringfunction. It can be set empirically, chosen based on operational criteria (e.g.,targetindexsize),or(forawell-calibratedrelevancescoringfunction)determineda priori. In this work, we combine the ﬁrst two strategies: we pick tbased onthe distribution of relevance scores across all expansion queries. For instance,atp= 0:3we only keep queries with relevance scores in the top 30%, which ist= 3:215for the ELECTRA [31] scoring model on the MS MARCO dataset [26].4 Experimental SetupWe conduct experiments to answer the following research questions:RQ1 Does Doc2Query-- improve the eﬀectiveness of document expansion?RQ2 What are the trade-oﬀs in terms of eﬀectiveness, eﬃciency, and storage whenusing Doc2Query--?Datasets and Measures. We conduct tests using the MS MARCO [26] v1passage corpus. We use ﬁve test collections:3(1) the MS MARCO Dev (small)collection, consisting of 6,980 queries (1.1 qrels/query); (2) the Dev2 collection,consisting of 4,281 (1.1 qrels/query); (3) the MS MARCO Eval set, consisting of6,837 queries (held-out leaderboard set); (4/5) the TREC DL’19/’20 collections,consisting of 43/54 queries (215/211 qrels/query). We evaluate using the oﬃcialtask evaluation measures: Reciprocal Rank at 10 (RR@10) for Dev/Dev2/Eval,nDCG@10 for DL’19/’20. We tune systems4on Dev, leaving the remaining col-lections as held-out test sets.Models. We use the T5 Doc2Query model from Nogueira and Lin [28], mak-ing use of the inferred queries released by the authors (80 per passage). To thebest of our knowledge, this is the highest-performing Doc2Query model avail-able. We consider three neural relevance models for ﬁltering: ELECTRA5[31],MonoT56[32],andTCT-ColBERT7[16],coveringtwostrongcross-encodermod-els and one strong bi-encoder model. We also explored ﬁlters that use the prob-abilities from the generation process itself but found them to be ineﬀective andtherefore omit these results due to space constraints.Tools and Environment. WeusethePyTerriertoolkit[22]withaPISA[24,17]index to conduct our experiments. We deploy PISA’s Block-Max WAND [7] im-plementation for BM25 retrieval. Inference was conducted on an NVIDIA 3090GPU. Evaluation was conducted using the ir-measures package [18].3ir-datasets [21] IDs: msmarco-passage/dev/small ,msmarco-passage/dev/2 ,msmarco-passage/eval/small , msmarco-passage/trec-dl-2019/judged ,msmarco-passage/trec-dl-2020/judged4BM25’s k1,b, and whether toremove stopwords were tuned for all systems; the ﬁltering percentage ( p)was also tuned for ﬁltered systems.5crystina-z/monoELECTRA_LCE_nneg316castorini/monot5-base-msmarco7castorini/tct_colbert-v2-hnp-msmarcoDoc2Query--: When Less is More 5Table 1. Eﬀectiveness and eﬃciency measurements for Doc2Query-- and baselines.Signiﬁcant diﬀerences between Doc2Query and their corresponding ﬁltered versionsfor Dev, Dev2, DL’19 and DL’20 are indicated with * (paired t-test, p < 0:05). Valuesmarked withyare taken from the corresponding submissions to the public leaderboard.RR@10 nDCG@10 ms/q GBSystem Dev Dev2 Eval DL’19 DL’20 MRT IndexBM25 0.185 0.182y0.186 0.499 0.479 5 0.71Doc2Query ( n= 40) 0.277 0.265y0.272 0.626 0.607 30 1.17w/ ELECTRA Filter (30%) *0.316 *0.310 -0.667 0.611 23 0.89w/ MonoT5 Filter (40%) *0.308 *0.298 0.306 0.650 0.611 29 0.93w/ TCT Filter (50%) *0.287 *0.280 - 0.640 0.599 30 0.94Doc2Query ( n= 80) 0.279 0.267 - 0.627 0.605 30 1.41w/ ELECTRA Filter (30%) *0.323 *0.316 0.325 0.670 0.614 23 0.95w/ MonoT5 Filter (40%) *0.311 *0.298 - 0.665 0.609 28 1.04w/ TCT Filter (50%) *0.293 *0.283 - 0.642 0.588 28 1.055 ResultsWe ﬁrst explore RQ1: whether relevance ﬁltering can improve the retrieval ofDoc2Query models. Table 1 compares the eﬀectiveness of Doc2Query with var-ious ﬁlters. We observe that all the ﬁlters signiﬁcantly improve the retrievaleﬀectiveness on the Dev and Dev2 datasets at both n= 40andn= 80. We alsoobserve a large boost in performance on the Eval dataset.8Though the diﬀer-ences in DL’19 and DL’20 appear to be considerable (e.g., 0.627 to 0.670), thesediﬀerences are not statistically signiﬁcant.Diggingalittledeeper,Figure2showstheretrievaleﬀectivenessofDoc2Querywith various numbers of generated queries (in dotted black) and the correspond-ing performance when ﬁltering using the top-performing ELECTRA scorer (insolid blue). We observe that performing relevance ﬁltering at each value of nimproves the retrieval eﬀectiveness. For instance, keeping only 30% of expan-sion queries at n= 80, performance is increased from 0.279 to 0.323 – a 16%improvement.In aggregate, results from Table 1 and Figure 2 answer RQ1: Doc2Query--ﬁltering can signiﬁcantly improve the retrieval eﬀectiveness of Doc2Query acrossvarious scoring models, numbers of generated queries ( n) and thresholds ( p).Next,weexplorethetrade-oﬀsintermsofeﬀectiveness,eﬃciency,andstoragewhen using Doc2Query--. Table 1 includes the mean response time and indexsizes for each of the settings. As expected, ﬁltering reduces the index size sincefewer terms are stored. For the best-performing setting ( n= 80with ELECTRA8Signiﬁcance cannot be determined due to the held-out nature of the dataset. Further,due to restrictions on the number of submissions to the leaderboard, we only are ableto submit two runs. The ﬁrst aims to be a fair comparison with the existing Doc2QueryEval result, using the same number of generated queries and same base T5 model forscoring. The second is our overall best-performing setting, using the ELECTRA ﬁlteratn= 80generated queries.6 Gospodinov et al.0 1 2 3 4 5Total Tokens 1e90.2250.2500.2750.3000.325RR@1090%80%70%60%50%40% 30%n=5n=10n=20n=40 n=80Generation PhaseFiltering PhaseFig. 2.Eﬀectiveness (RR@10) on the Dev set, compared with the total number ofindexed tokens. The generation phase is shown in dotted black (at various values ofn), and the ELECTRA ﬁltering phase is shown in solid blue (at various values of p).ﬁlter), this amounts to a 33% reduction in index size (1.41 GB down to 0.95 GB).Naturally, such a reduction has an impact on query processing time as well; ityields a 23% reduction in mean response time (30ms down to 23ms).Doc2Query-- ﬁltering adds substantial cost an indexing time, mostly due toscoring each of the generated queries. Table 2 reports the cost (in hours of GPUtime) of the generation and ﬁltering phases. We observe that ELECTRA ﬁlter-ing can yield up to a 78% increase in GPU time ( n= 10). However, we ﬁnd thatthe improved eﬀectiveness makes up for this cost. To demonstrate this, we al-locate the time spent ﬁltering to generating additional queries for each passage.For instance, the 15 hours spent scoring n= 5queries could instead be spentgenerating 6 more queries per passage (for a total of n= 11). We ﬁnd that whencomparing against an unﬁltered nthat closely approximates the total time whenTable 2. Retrieval eﬀectiveness comparison for comparable indexing computationalbudgets (in hours of GPU time). Values of nwithout a ﬁlter are chosen to best approx-imate the total compute hours or the Dev eﬀectiveness of the corresponding ﬁlteredversion. Signiﬁcant diﬀerences between in RR@10 performance are indicated with *(paired t-test, p < 0:05).GPU Hours RR@10nFilter Gen+Filt=Tot Dev Dev2 Comment5 ELECTRA 20 + 15 = 34 0.273 0.27011None 34 + 0 = 34 *0.261 *0.256  4% Dev RR for sim. GPU hrs31None 99 + 0 = 99 0.273 0.2652:9GPU hrs to match Dev RR10 ELECTRA 32 + 25 = 57 0.292 0.29218None 59 + 0 = 59 *0.270 *0.260  8% Dev RR for sim. GPU hrs20 ELECTRA 66 + 47 = 113 0.307 0.30336None 113 + 0 = 113 *0.275 *0.265  10% Dev RR for sim. GPU hrs40 ELECTRA 128 + 86 = 214 0.316 0.31068None 216 + 0 = 216 *0.279 *0.267  12% Dev RR for sim. GPU hrsDoc2Query--: When Less is More 7ﬁltering, the ﬁltered results consistently yield signiﬁcantly higher retrieval eﬀec-tiveness. As the computational budget increases, so does the margin betweenDoc2Query and Doc2Query--, from 4% at 34 hours up to 12% at 216 hours.From the opposite perspective, Doc2Query consumes 2.9 or more GPUtime than Doc2Query-- to achieve similar eﬀectiveness ( n= 13with no ﬁltervs.n= 5with ELECTRA ﬁlter). Since the eﬀectiveness of Doc2Query ﬂattensout between n= 40andn= 80(as seen in Figure 2), it likely requires amassive amount of additional compute to reach the eﬀectiveness of Doc2Query--atn10, if that eﬀectiveness is achievable at all. These comparisons show thatif a deployment is targeting a certain level of eﬀectiveness (rather than a targetcompute budget), Doc2Query-- is also preferable to Doc2Query.TheseresultscollectivelyanswerRQ2:Doc2Query--provideshighereﬀective-ness at lower query-time costs, even when controlling for the additional computerequired at index time.6 ConclusionsThisworkdemonstratedthatthereareuntappedadvantagesingeneratingnatural-language for document expansion. Speciﬁcally, we presented Doc2Query--, whichisanewapproachforimprovingtheeﬀectivenessandeﬃciencyoftheDoc2Querymodel by ﬁltering out the least relevant queries. We observed that a 16% im-provement in retrieval eﬀectiveness can be achieved, while reducing the indexsize by 33% and mean query execution time by 23%.The technique of ﬁltering text generated from language models using rel-evance scoring is ripe for future work. For instance, relevance ﬁltering couldpotentially apply to approaches that generate alternative forms of queries [38],training data [2], or natural language responses to queries [5] — all of whichare potentially aﬀected by hallucinated content. Furthermore, future work couldexplore approaches for relevance ﬁltering over masked language modelling ex-pansion [19], rather than sequence-to-sequence expansion.AcknowledgementsSeanMacAvaneyandCraigMacdonaldacknowledgeEPSRCgrantEP/R018634/1:Closed-Loop Data Science for Complex, Computationally- & Data-Intensive An-alytics.References1. Amati, G., Van Rijsbergen, C.J.: Probabilistic models of information retrievalbased on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20(4)(2002)2. Bonifacio,L.,Abonizio,H.,Fadaee,M.,Nogueira,R.:InPars:Unsuperviseddatasetgeneration for information retrieval. In: Proceedings of SIGIR (2022)8 Gospodinov et al.3. Dai, Z., Callan, J.: Deeper text understanding for IR with contextual neural lan-guage modeling. In: Proceedings of SIGIR (2019)4. Dai, Z., Callan, J.: Context-aware document term weighting for ad-hoc search. In:Proceedings of The Web Conference (2020)5. Das, R., Dhuliawala, S., Zaheer, M., McCallum, A.: Multi-step retriever-readerinteraction for scalable open-domain question answering. In: Proceedings of ICLR(2019)6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deepbidirectional transformers for language understanding. In: Proceedings of NAACL-HLT (2019)7. Ding, S., Suel, T.: Faster top-k document retrieval using block-max indexes. In:Proceedings of SIGIR (2011)8. Dumais, S.T., Furnas, G.W., Landauer, T.K., Deerwester, S., Harshman, R.: Usinglatent semantic analysis to improve access to textual information. In: Proceedingsof SIGCHI CHI (1988)9. Efron, M., Organisciak, P., Fenlon, K.: Improving retrieval of short texts throughdocument expansion. In: Proceedings of SIGIR (2012)10. Formal, T., Piwowarski, B., Clinchant, S.: SPLADE: Sparse lexical and expansionmodel for ﬁrst stage ranking. In: Proceedings of SIGIR (2021)11. He, B., Ounis, I.: Studying query expansion eﬀectiveness. In: Proceedings of ECIR(2009)12. Jaleel, N.A., Allan, J., Croft, W.B., Diaz, F., Larkey, L.S., Li, X., Smucker, M.D.,Wade, C.: Umass at TREC 2004: Novelty and HARD. In: TREC (2004)13. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. IEEETransactions on Big Data 7(03) (2021)14. Khattab, O., Zaharia, M.: ColBERT: Eﬃcient and eﬀective passage search viacontextualized late interaction over BERT. In: Proceedings of SIGIR (2020)15. Lin, J., Ma, X., Mackenzie, J., Mallia, A.: On the separation of logical and physicalranking models for text retrieval applications. In: Proceedings of DESIRES (2021)16. Lin, S.C., Yang, J.H., Lin, J.: In-batch negatives for knowledge distillation withtightly-coupled teachers for dense retrieval. In: Proceedings of RepL4NLP (2021)17. MacAvaney, S., Macdonald, C.: A Python interface to PISA! In: Proceedings ofSIGIR (2022)18. MacAvaney,S.,Macdonald,C.,Ounis,I.:Streamliningevaluationwithir-measures.In: Proceedings of ECIR (2022)19. MacAvaney, S., Nardini, F.M., Perego, R., Tonellotto, N., Goharian, N., Frieder,O.: Expansion via prediction of importance with contextualization. In: Proceedingsof SIGIR (2020)20. MacAvaney, S., Tonellotto, N., Macdonald, C.: Adaptive re-ranking with a corpusgraph. In: Proceedings of CIKM (2022)21. MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian, N.:Simpliﬁed data wrangling with ir_datasets. In: Proceedings of SIGIR (2021)22. Macdonald, C., Tonellotto, N.: Declarative experimentation in information re-trieval using PyTerrier. In: Proceedings of ICTIR (2020)23. Mallia, A., Khattab, O., Suel, T., Tonellotto, N.: Learning passage impacts forinverted indexes. In: Proceedings of SIGIR (2021)24. Mallia, A., Siedlaczek, M., Mackenzie, J., Suel, T.: PISA: performant indexes andsearch for academia. In: Proceedings of OSIRRC@SIGIR (2019)25. Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factualityin abstractive summarization. In: Proceedings of ACL (2020)Doc2Query--: When Less is More 926. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,L.: MS MARCO: A human generated machine reading comprehension dataset. In:Proceedings of CoCo@NIPS (2016)27. Nogueira, R., Cho, K.: Passage re-ranking with BERT. ArXiv abs/1901.04085(2019)28. Nogueira, R., Lin, J.: From doc2query to doctttttquery (2019)29. Nogueira, R., Yang, W., Lin, J.J., Cho, K.: Document expansion by query predic-tion. ArXiv abs/1904.08375 (2019)30. Pickens, J., Cooper, M., Golovchinsky, G.: Reverted indexing for feedback andexpansion. In: Proceedings of CIKM (2010)31. Pradeep, R., Liu, Y., Zhang, X., Li, Y., Yates, A., Lin, J.: Squeezing water from astone: A bag of tricks for further improving cross-encoder eﬀectiveness for rerank-ing. In: Proceedings of ECIR (2022)32. Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for textranking with pretrained sequence-to-sequence models. ArXiv abs/2101.05667(2021)33. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,Li, W., Liu, P.J., et al.: Exploring the limits of transfer learning with a uniﬁedtext-to-text transformer. J. Mach. Learn. Res. 21(140) (2020)34. Scells, H., Zhuang, S., Zuccon, G.: Reduce, reuse, recycle: Green information re-trieval research. In: Proceedings of SIGIR (2022)35. Tao, T., Wang, X., Mei, Q., Zhai, C.: Language model information retrieval withdocument expansion. In: Proceedings of HLT-NAACL (2006)36. Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: An inspection of the repro-ducibility and replicability of TCT-ColBERT. In: Proceedings of SIGIR (2022)37. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Over-wijk,A.:Approximatenearestneighbornegativecontrastivelearningfordensetextretrieval. In: Proceedings of ICLR (2021)38. Yu, S.Y., Liu, J., Yang, J., Xiong, C., Bennett, P.N., Gao, J., Liu, Z.: Few-shotgenerative conversational query rewriting. In: Proceedings of SIGIR (2020)39. Zhao, T., Lu, X., Lee, K.: SPARTA: Eﬃcient open-domain question answering viasparse transformer matching retrieval. arXiv abs/2009.13013 (2020)40. Zhuang, S., Zuccon, G.: TILDE: Term independent likelihood model for passagere-ranking. In: Proceedings of SIGIR (2021)