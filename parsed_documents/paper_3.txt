A Few Brief Notes on DeepImpact, COIL, and a ConceptualFramework for Information Retrieval TechniquesJimmy Lin andXueguang MaDavid R. Cheriton School of Computer ScienceUniversity of WaterlooAbstractRecent developments in representational learn-ing for information retrieval can be organizedin a conceptual framework that establishes twopairs of contrasts: sparse vs. dense representa-tions and unsupervised vs. learned representa-tions. Sparse learned representations can fur-ther be decomposed into expansion and termweighting components. This framework al-lows us to understand the relationship betweenrecently proposed techniques such as DPR,ANCE, DeepCT, DeepImpact, and COIL, andfurthermore, gaps revealed by our analysispoint to “low hanging fruit” in terms of tech-niques that have yet to be explored. We presenta novel technique dubbed “uniCOIL”, a simpleextension of COIL that achieves to our knowl-edge the current state-of-the-art in sparse re-trieval on the popular MS MARCO passageranking dataset. Our implementation usingthe Anserini IR toolkit is built on the Lucenesearch library and thus fully compatible withstandard inverted indexes.1 IntroductionWe present a novel conceptual framework for un-derstanding recent developments in information re-trieval that organizes techniques along two dimen-sions. The ﬁrst dimension establishes the contrastbetween sparse and dense vector representationsfor queries and documents.1The second dimen-sion establishes the contrast between unsupervisedand learned (supervised) representations. Figure 1illustrates our framework.Recent proposals for dense retrieval, exempliﬁedby DPR (Karpukhin et al., 2020) and ANCE (Xionget al., 2021), but also encompassing many othertechniques (Gao et al., 2021b; Hofstätter et al.,2020; Qu et al., 2021; Hofstätter et al., 2021; Lin1Consistent with parlance in information retrieval, we use“document” throughout this paper in a generic sense to refer tothe unit of retrieved text. To be more precise, our experimentsare in fact focused on passage retrieval.Dense SparseSupervised DPR, ANCE DeepImpact, COILUnsupervised LSI, LDA BM25, tf–idfTable 1: Our conceptual framework for organizing re-cent developments in information retrieval.et al., 2021), can be understood as learned denserepresentations for retrieval. This is formulatedas a representational learning problem where thetask is to learn (transformer-based) encoders thatmap queries and documents into dense ﬁxed-widthvectors (768 dimensions is typical) in which innerproducts between queries and relevant documentsare maximized, based on supervision signals froma large dataset such as the MS MARCO passageranking test collection (Bajaj et al., 2018). See Linet al. (2020) for a survey.Dense retrieval techniques are typically com-pared against a bag-of-words exact match rankingmodel such as BM25, which in this context can beunderstood as unsupervised sparse retrieval. Al-though it may be unnatural to describe BM25 inthis way, it is technically accurate: each documentis represented by a sparse vector where each dimen-sion corresponds to a unique term in the vocabulary,and the scoring function assigns a weight to each di-mension. As with dense retrieval, query–documentscores are computed via inner products.What about learned sparse retrieval? The mostprominent recent example of this in the literatureis DeepCT (Dai and Callan, 2019), which usesa transformer to learn term weights based on a re-gression model, with the supervision signal comingfrom the MS MARCO passage ranking test collec-tion.2DeepCT has an interesting “quirk”: in truth,it only learns the term frequency (tf) componentof term weights, but still relies on the remaining2Learning sparse representations is by no means a new idea.The earliest example we are aware of is Wilbur (2001), whoattempted to learn global term weights using TREC data, butthe idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-eration of pseudo-documents. This approach alsohas a weakness: it only assigns weights to termsthat are already present in the document, whichlimits retrieval to exact match. This is an impor-tant limitation that is addressed by the use of denserepresentations, which are capable of capturing se-mantic matches.These two issues were resolved by the recentlyproposed DeepImpact model (Mallia et al., 2021),which also belongs in the family of learned sparserepresentations. DeepImpact brought together twokey ideas: the use of document expansion to iden-tify dimensions in the sparse vector that shouldhave non-zero weights and a term weighting modelbased on a pairwise loss between relevant and non-relevant texts with respect to a query. Expansionterms were identiﬁed by doc2query–T5 (Nogueiraand Lin, 2019), a sequence-to-sequence model fordocument expansion that predicts queries for whicha text would be relevant. Since the DeepImpactscoring model directly predicts term weights thatare then quantized, it would be more accurate tocall these weights learned impacts, since query–document scores are simply the sum of weights ofdocument terms that are found in the query. Callingthese impact scores draws an explicit connection toa thread of research in information retrieval datingback two decades (Anh et al., 2001).The recently proposed COIL architecture (Gaoet al., 2021a) presents an interesting case for thisconceptual framework. Where does it belong? Theauthors themselves describe COIL as “a new ex-act lexical match retrieval architecture armed withdeep LM representations”. COIL produces repre-sentations for each document token that are thendirectly stored in the inverted index, where theterm frequency usually goes in an inverted list.Although COIL is perhaps best described as theintellectual descendant of ColBERT (Khattab andZaharia, 2020), another way to think about it withinour conceptual framework is that instead of assign-ingscalar weights to terms in a query, the “scoring”model assigns each term a vector “weight”. Queryevaluation in COIL involves accumulating innerproducts instead of scalar weights.Our conceptual framework highlights a ﬁnalclass of techniques: unsupervised dense represen-tations. While there is little work in this space oflate, it does describe techniques such as LSI (Deer-wester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-ously explored. Thus, all quadrants in our proposedconceptual framework are populated with knownexamples from the literature.2 Comments and ObservationsBased on this framework, we can make a number ofinteresting observations that highlight obvious nextsteps in the development of retrieval techniques.We discuss as follows:Choice of bases. Retrieval techniques using learneddense representations and learned sparse represen-tations present an interesting contrast. Nearly allrecent proposals take advantage of transformers, sothat aspect of the design is not a salient difference.The critical contrast is the basis of the vector rep-resentations: In sparse approaches, the basis of thevector space remains ﬁxed to the corpus vocabulary,and thus techniques such as DeepCT, COIL, andDeepImpact can be understood as term weightingmodels. In dense approaches, the model is giventhe freedom to choose a new basis derived fromtransformer representations. This change in basisallows the encoder to represent the “meaning” oftexts in relatively small ﬁxed-width vectors (com-pared to sparse vectors that may have millions ofdimensions). This leads us to the next importantobservation:Expansions for sparse representation. Withoutsome form of expansion, learned sparse represen-tations remain limited to (better) exact matchingbetween queries and documents. The nature ofsparse representations means that it is impracticalto consider non-zero weights for allelements inthe vector (i.e., the vocabulary space). Thus, docu-ment expansion serves the critical role of proposinga set of candidate terms that should receive non-zero weights; since the number of candidate termsis small compared to the vocabulary size, the re-sulting vector remains sparse. Without expansion,learned sparse representations cannot address thevocabulary mismatch problem (Furnas et al., 1987),because document terms not present in the querycannot contribute any score. For DeepImpact, thisexpansion is performed by doc2query–T5, but inprinciple we can imagine other methods also. Thisleads us to the next important observation:Relating DeepCT, DeepImpact, and COIL. The up-shot of the above analysis is that retrieval tech-niques based on learned sparse representationsshould be divided into an expansion model andSparse Representations MRR@10 NotesTerm Weighting Expansion(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure(2c) DeepImpact None ? no publicly reported ﬁgure(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment(2g) uniCOIL None 0.315 our experiment(2h) uniCOIL doc2query–T5 0.352 our experimentDense Representations MRR@10 Notes(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)(3b) ANCE 0.330 copied from (Xiong et al., 2021)(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)(3d) RocketQA 0.370 copied from (Qu et al., 2021)(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)Dense–Sparse Hybrids MRR@10 Notes(4a) CLEAR 0.338 copied from (Gao et al., 2021b)(4b) COIL-full 0.355 copied from (Gao et al., 2021a)(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experimentTable 2: Results on the development queries of the MS MARCO passage ranking task.a term weighting model. For example, DeepCTperforms no expansion and uses a regression-basedscoring model. DeepImpact performs document ex-pansion and uses a pairwise scoring model. COILperforms no expansion and uses a “scoring” modelthat generates a contextualized “weight vector” (in-stead of a scalar weight). This breakdown suggestsa number of obvious experiments that help us un-derstand the contributions of these components,which we report next.3 ExperimentsOur proposed conceptual framework can be usedto organize results from the literature, which areshown in Table 2 on the development queries ofthe MS MARCO passage ranking task (Bajaj et al.,2018). Some of these entries represent ﬁgures di-rectly copied from previous papers (with referencesshown), while others are novel experimental condi-tions that we report.The ﬁrst main block of the table shows retrievalwith sparse representations. Row (1a) shows theBM25 baseline, and row (1b) provides the effective-ness of doc2query–T5 expansion. In both cases, theterm weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrievaltechniques are shown in row group (2). Separat-ing the term weighting component from the ex-pansion component allows us to identify gaps inmodel conﬁgurations that would be interesting toexplore. For example, in row (2a), DeepCT pro-posed a regression-based term weighting model,but performed no expansion. However, the termweighting model can be applied to expanded doc-uments, as in row (2b); to our knowledge, thisconﬁguration has not been publicly reported.Similarly, DeepImpact combined doc2query–T5as an expansion model and a term weighting modeltrained with pairwise loss. To better understandthe contributions of each component, we couldrun the term weighting model without documentexpansion, as outlined in row (2c). This ablationexperiment was not reported in Mallia et al. (2021),but would be interesting to conduct.In row (2e) we report the published results ofCOIL-tok (token dimension d= 32 ), which is thesparse component in the full COIL model (whichis a dense–sparse hybrid). Through the lens ofour conceptual framework, a number of extensionsbecome immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-vided by the authors,3we trained such a modelfrom scratch, using the same hyperparameters asthe authors. This variant leads to a nearly two-pointgain in effectiveness, as shown in row (2f).In another interesting extension, if we reduce thetoken dimension of COIL to one, the model degen-erates into producing scalar weights, which thenbecomes directly comparable to DeepCT, row (2a)and the “no-expansion” variant of DeepImpact, row(2c). These comparisons isolate the effects of differ-ent term weighting models. We dub this variant ofCOIL “uniCOIL”, on top of which we can also adddoc2query–T5, which produces a fair comparisonto DeepImpact, row (2d). The original formulationof COIL, even with a token dimension of one, isnot directly amenable to retrieval using invertedindexes because weights can be negative. To ad-dress this issue, we added a ReLU operation onthe output term weights of the base COIL model toforce the model to generate non-negative weights.Once again, we retrained the model from scratchusing the same hyperparameters provided by theauthors. When encoding the corpus, we quantizedthese weights into 8 bits to obtain impact scores;query weights are similarly quantized. After thesemodiﬁcations, uniCOIL is directly compatible withinverted indexes. Our experimental results are re-ported with the Anserini toolkit (Yang et al., 2017,2018), which is built on Lucene.It is no surprise that uniCOIL without doc2query–T5, row (2g), is less effective than COIL-tok ( d=32), row (2e). However, uniCOIL with doc2query–T5, row (2h), outperforms COIL-tok without need-ing any specialized retrieval infrastructure—theweights are just impact scores, like in DeepImpact.These results suggest that contextualized “weightvectors” in COIL aren’t necessary to achieve goodeffectiveness—adding expansion appears sufﬁcientto make up for the lost expressivity of weight vec-tors, as shown in row (2h) vs. row (2e). To ourknowledge, our uniCOIL model, row (2h), repre-sents the state of the art in sparse retrieval usinglearned impact weights, beating DeepImpact byaround two points.The second main block of Table 2 provides anumber of comparable dense retrieval results fromthe literature. The highest score that we are awareof is RocketQA (Qu et al., 2021), whose effective-ness beats all known sparse conﬁgurations. Note3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) usesthe more expressive MaxSim operator to comparequery and document representations; all other tech-niques use inner products.The ﬁnal block of Table 2 presents the results ofdense–sparse hybrids. Lin et al. (2021) reportedthe results of dense–sparse hybrids when TCT-ColBERTv2, row (3f), is combined with BM25,row (1a), and doc2query–T5, row (1b). To this,we added fusion with DeepImpact, uniCOIL, andCOIL-tok (d= 32 ). For a fair comparison, we fol-lowed the same technique for combining dense andsparse results as Lin et al. (2021), which is from Maet al. (2021). For each query q, we used the corre-sponding dense and sparse techniques to retrievetop-1k documents. The ﬁnal fusion score of eachdocument is calculated by sdense +ssparse . Sincethe range of the two different scores are quite differ-ent, we ﬁrst normalized the scores into range(0, 1).Thewas tuned in the range(0, 2) with a simpleline search on a subset of the MS MARCO passagetraining set.With these hybrid combinations, we are ableto achieve, to our knowledge, the highest reportedscores on the MS MARCO passage ranking task forsingle-stage techniques (i.e., no reranking). Notethat, as before, uniCOIL is compatible with stan-dard inverted indexes, unlike COIL-tok, which re-quires custom infrastructure.4 Next StepsIn most recent work, dense retrieval techniques arecompared to BM25 and experiments show that theyhandily win. However, this is not a fair compari-son, since BM25 is unsupervised, whereas denseretrieval techniques exploit supervised relevancesignals from large datasets. A more appropriatecomparison would be between learned dense vs.sparse representations—and there, no clear win-ner emerges at present. However, it seems clearthat they are complementary, as hybrid approachesappear to be more effective than either alone.An important point to make here is that neu-ral networks, particularly transformers, have notmade sparse representations obsolete. Both denseand sparse learned representations clearly exploittransformers—the trick is that the latter class oftechniques then “projects” the learned knowledgeback into the sparse vocabulary space. This al-lows us to reuse decades of innovation in invertedindexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluationalgorithms (e.g., smart skipping to reduce querylatency): for example, the Lucene index used inour uniCOIL experiments is only 1.3 GB, com-pared to 40 GB for COIL-tok, 26 GB for TCT-ColBERTv2, and 154 GB for ColBERT. We note,however, that with dense retrieval techniques, ﬁxed-width vectors can be approximated with binaryhash codes, yielding far more compact representa-tions with sacriﬁcing much effectiveness (Yamadaet al., 2021). Once again, no clear winner emergesat present.The complete design space of modern informa-tion retrieval techniques requires proper accountingof the tradeoffs between output quality (effective-ness), time (query latency), and space (index size).Here, we have only focused on the ﬁrst aspect.Learned representations for information retrievalare clearly the future, but the advantages and dis-advantages of dense vs. sparse approaches alongthese dimensions are not yet fully understood. It’llbe exciting to see what comes next!5 AcknowledgmentsThis research was supported in part by the CanadaFirst Research Excellence Fund and the Natural Sci-ences and Engineering Research Council (NSERC)of Canada. Computational resources were providedby Compute Ontario and Compute Canada.ReferencesV o Ngoc Anh, Owen de Kretser, and Alistair Moffat.2001. Vector-space ranking with effective early ter-mination. In Proceedings of the 24th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 2001) ,pages 35–42, New Orleans, Louisiana.Avinash Atreya and Charles Elkan. 2010. Latent se-mantic indexing (LSI) fails for TREC collections.SIGKDD Explorations , 12(2):5–10.Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder,Andrew McNamara, Bhaskar Mitra, Tri Nguyen,Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-wary, and Tong Wang. 2018. MS MARCO: A Hu-man Generated MAchine Reading COmprehensionDataset. arXiv:1611.09268v3 .Zhuyun Dai and Jamie Callan. 2019. Context-awaresentence/passage term importance estimation forﬁrst stage retrieval. arXiv:1910.10687 .Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal ofthe Association for Information Science , 41(6):391–407.George W. Furnas, Thomas K. Landauer, Louis M.Gomez, and Susan T. Dumais. 1987. The vo-cabulary problem in human-system communication.Communications of the ACM , 30(11):964–971.Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.COIL: Revisit exact lexical match in informationretrieval with contextualized inverted list. In Pro-ceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies , pages3030–3042.Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-jamin Van Durme, and Jamie Callan. 2021b. Com-plementing lexical retrieval with semantic residualembedding. In Proceedings of the 43rd EuropeanConference on Information Retrieval (ECIR 2021),Part I , pages 146–160.Sebastian Hofstätter, Sophia Althammer, MichaelSchröder, Mete Sertkan, and Allan Hanbury.2020. Improving efﬁcient neural ranking mod-els with cross-architecture knowledge distillation.arXiv:2010.02666 .Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-HongYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-ﬁciently teaching an effective dense retriever withbalanced topic aware sampling. In Proceedings ofthe 44th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval (SIGIR 2021) .Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval foropen-domain question answering. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP) , pages 6769–6781.Omar Khattab and Matei Zaharia. 2020. ColBERT: Ef-ﬁcient and effective passage search via contextual-ized late interaction over BERT. In Proceedings ofthe 43rd International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR 2020) , pages 39–48.Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.2020. Pretrained transformers for text ranking:BERT and beyond. arXiv:2010.06467 .Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021. In-batch negatives for knowledge distillationwith tightly-coupled teachers for dense retrieval. InProceedings of the 6th Workshop on RepresentationLearning for NLP .Xueguang Ma, Kai Sun, Ronak Pradeep, and JimmyLin. 2021. A replication study of dense passage re-triever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, andNicola Tonellotto. 2021. Learning passage impactsfor inverted indexes. In Proceedings of the 44th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR 2021) .Rodrigo Nogueira and Jimmy Lin. 2019. Fromdoc2query to docTTTTTquery.Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, RuiyangRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,and Haifeng Wang. 2021. RocketQA: An opti-mized training approach to dense passage retrievalfor open-domain question answering. In Proceed-ings of the 2021 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies , pages5835–5847.Xing Wei and W. Bruce Croft. 2006. LDA-based doc-ument models for ad-hoc retrieval. In Proceedingsof the 29th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR 2006) , pages 178–185, Seattle,Washington.W. John Wilbur. 2001. Global term weights for docu-ment retrieval learned from TREC data. Journal ofInformation Science , 27(5):303–310.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In Proceedings of the 9th International Con-ference on Learning Representations (ICLR 2021) .Ikuya Yamada, Akari Asai, and Hannaneh Ha-jishirzi. 2021. Efﬁcient passage retrieval withhashing for open-domain question answering.arXiv:2106.00882 .Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:enabling the use of Lucene for information retrievalresearch. In Proceedings of the 40th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 2017) ,pages 1253–1256, Tokyo, Japan.Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:reproducible ranking baselines using Lucene. Jour-nal of Data and Information Quality , 10(4):Article16.